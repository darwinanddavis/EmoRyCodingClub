colour=colpal
)) +
geom_line(show.legend=T)
p
ff <- flights %>%
mutate(date = time_hour %>% as.Date()) %>%
select(distance,arr_delay,date)
# make calendar df
fdf <- frame_calendar(ff,
x = distance,
y = arr_delay,
date = date,
nrow = 5)
# plot
p <- ggplot(fdf,aes(x = .distance,
y = .arr_delay,
group = date,
colour=colpal
)) +
geom_line(show.legend=T) +
theme_void()
p %>% prettify() # add calendar text
p <- ggplot(fdf,aes(x = .distance, # new calendar var
y = .arr_delay, # new calendar var
group = date,
colour=arr_delay
)) +
geom_line(show.legend=T) +
theme_void()
p %>% prettify() # add calendar text
rmarkdown::render_site()
pacman::p_load(maps,stringr,leaflet)
# get city names
data(world.cities) # /maps
world_cities <- world.cities
ot <- OlsonNames() %>% str_split_fixed("/",n = 2)  # sep country and cities
ot2 <- ot[,2] %>% str_replace_all("_"," ")
odf <- world_cities %>%
filter(name %in% ot2) %>%
select(lat = lat,
lon = long,
city = name)
# plot
leaflet() %>% addTiles() %>%
addMarkers(lng=odf$lon,lat=odf$lat,
label = paste0(odf$city,"</br>",
"Your current time =", now(), "</br>")) %>% leaflet::addProviderTiles("CartoDB")
# possible time zones to pass to the `tz=""` argument
odf[str_detect(OlsonNames(),ot[,2]),]
odf[complete.cases(odf[str_detect(OlsonNames(),ot[,2]),]),"city"]
pacman::p_load(maps,stringr,leaflet)
# get city names
data(world.cities) # /maps
world_cities <- world.cities
ot <- OlsonNames() %>% str_split_fixed("/",n = 2)  # sep country and cities
ot2 <- ot[,2] %>% str_replace_all("_"," ")
odf <- world_cities %>%
filter(name %in% ot2) %>%
select(lat = lat,
lon = long,
city = name)
# plot
leaflet() %>% addTiles() %>%
addMarkers(lng=odf$lon,lat=odf$lat,
label = paste(odf$city,"</br>",
"Your current time =", now(), "</br>")) %>% leaflet::addProviderTiles("CartoDB")
# possible time zones to pass to the `tz=""` argument
odf[str_detect(OlsonNames(),ot[,2]),]
odf[complete.cases(odf[str_detect(OlsonNames(),ot[,2]),]),"city"]
ot2
ot
ot2
odf <- world_cities %>%
filter(name %in% ot2) %>%
select(lat = lat,
lon = long,
city = name,
tz = now(tzone=ot2))
ot2
now(tzone = ot2)
ot2 %>% class
ot2
ot2[1]
now(tzone = ot2[1])
?now()
ot
now(OlsonNames[1])
now(OlsonNames()[1])
data(world.cities) # /maps
world_cities <- world.cities
ot <- OlsonNames() %>% str_split_fixed("/",n = 2)  # sep country and cities
ot2 <- ot[,2] %>% str_replace_all("_"," ")
odf <- world_cities %>%
filter(name %in% ot2) %>%
select(lat = lat,
lon = long,
city = name,
tz = now(OlsonNames()))
OlsonNames()
now(OlsonNames())
OlsonNames()
now(OlsonNames()[2])
now(OlsonNames()[200])
odf %>% lengths
ot
ot2 %>% str_which(OlsonNames())
OlsonNames() %>% str_which(ot2)
pacman::p_load(maps,stringr,leaflet)
# get city names
data(world.cities) # /maps
world_cities <- world.cities
ot <- OlsonNames() %>% str_split_fixed("/",n = 2)  # sep country and cities
ot2 <- ot[,2] %>% str_replace_all("_"," ")
odf <- world_cities %>%
filter(name %in% ot2) %>%
select(lat = lat,
lon = long,
city = name)
# plot
leaflet() %>% addTiles() %>%
addMarkers(lng=odf$lon,lat=odf$lat,
label = paste("City: ",odf$city) %>% leaflet::addProviderTiles("CartoDB")
# possible time zones to pass to the `tz=""` argument
odf[str_detect(OlsonNames(),ot[,2]),]
pacman::p_load(maps,stringr,leaflet)
# get city names
data(world.cities) # /maps
world_cities <- world.cities
ot <- OlsonNames() %>% str_split_fixed("/",n = 2)  # sep country and cities
ot2 <- ot[,2] %>% str_replace_all("_"," ")
odf <- world_cities %>%
filter(name %in% ot2) %>%
select(lat = lat,
lon = long,
city = name)
# plot
leaflet() %>% addTiles() %>%
addMarkers(lng=odf$lon,lat=odf$lat,
label = paste("City: ",odf$city)) %>% leaflet::addProviderTiles("CartoDB")
# possible time zones to pass to the `tz=""` argument
odf[str_detect(OlsonNames(),ot[,2]),]
odf[complete.cases(odf[str_detect(OlsonNames(),ot[,2]),]),"city"]
rmarkdown::render_site()
node("section") %>% html_nodes("a") %>% html_attr("href")
node("img")  %>% html_attr("src") %>% image_read()
require(dplyr)
node("img") %>% html_attr("src") %>% image_read()
node("img") %>% html_attr("src")
?extract2
url %>% html_nodes("a") %>% extract2(3)
require(rvest)
url %>% html_nodes("a") %>% extract2(3)
url %>% html_nodes("a")
url %>% read_html %>% html_nodes("a")
rmarkdown::render_site()
?read_html
??read_html
rmarkdown::render_site()
require(rvest)
require(xml2)
url <- "https://r4ds.had.co.nz/data-visualisation.html" # url to scrape
url %>% read_html # scrape HTML text and data
# scraping the first node only for third order headings ("h3")
url %>% read_html %>% html_node("h3") # get only first section heading
# scraping all nodes in the webpage
url %>% read_html %>% html_nodes("h3") # get section headings
url %>% read_html %>% html_nodes("img") # get just images
node("h1") # get first order headings
node <- function(n){
url %>% read_html %>% html_nodes(n)  # n = user defined node
}
node("h1") # get first order headings
# now pull just the text without the HTML elements
node("h2") %>% html_text(trim=T)
node("strong") %>% html_text(trim=T) # get just bold text
node("code") %>% html_text(trim = T) %>% unique %>% head(10)
node("blockquote") %>% html_text(trim = T)
url <- "https://www.imdb.com/title/tt0094625/?ref_=fn_al_tt_1"
url %>% read_html %>% html_table(trim = T)  # get just tables
url <- "https://r4ds.had.co.nz/index.html"
node("a") %>% head(20) # return first 20 results
node("section") %>% html_nodes("a") %>% html_attr("href") %>% head(20) # return first 20 results
node("section") %>% html_nodes("a") %>% html_attr("href")
node("section") %>% html_nodes("a")
node("section")
node("a") %>% head(20) # return first 20 results
node("section")
node("a")
node("a") %>% html_attr("href") %>% head(20) #
node("section") %>% html_nodes("a") %>% html_text() %>% head(20) # return first 20 results
require(rvest)
require(xml2)
url <- "https://r4ds.had.co.nz/data-visualisation.html" # url to scrape
url %>% read_html # scrape HTML text and data
# scraping the first node only for third order headings ("h3")
url %>% read_html %>% html_node("h3") # get only first section heading
# scraping all nodes in the webpage
url %>% read_html %>% html_nodes("h3") # get section headings
url %>% read_html %>% html_nodes("img") # get just images
node <- function(n){
url %>% read_html %>% html_nodes(n)  # n = user defined node
}
node <- function(n){
url %>% read_html %>% html_nodes(n)  # n = user defined node
}
node("h1") # get first order headings
# now pull just the text without the HTML elements
node("h2") %>% html_text(trim=T)
node("strong") %>% html_text(trim=T) # get just bold text
node("section") %>% html_nodes("a") %>% html_attr("href") %>% head(20) # return first 20 results
node <- function(n){
url %>% read_html %>% html_nodes(n)  # n = user defined node
}
node("section") %>% html_nodes("a") %>% html_attr("href") %>% head(20) # return first 20 results
url
node("level1") %>% html_nodes("a") %>% html_attr("href") %>% head(20) # return first 20 results
node("content") %>% html_nodes("a") %>% html_attr("href") %>% head(20) # return first 20 results
node("body") %>% html_nodes("a") %>% html_attr("href") %>% head(20) # return first 20 results
node("a") %>% head(20) # return first 20 results
url <- "https://r4ds.had.co.nz/index.html"
node("a") %>% head(20) # return first 20 results
node("section") %>% html_nodes("a") %>% html_attr("href") %>% head(20) # return first 20 results
url <- "https://r4ds.had.co.nz/index.html"
node("a") %>% head(20) # return first 20 results
url <- "https://r4ds.had.co.nz/index.html"
node("section") %>% html_nodes("a") %>% html_attr("href") %>% head(20)
url <- "https://r4ds.had.co.nz/index.html"
node("section") %>% html_nodes("a") %>% html_attr("href") %>% head(20) # return first 20 results
node("section")
node("a") %>% head(20) # return first 20 results
node("a") %>%
html_attr("href") %>% head(20) #
node("a") %>% html_nodes("a") %>% html_attr("href") %>% head(20) # return first 20 results
node("section") %>% html_nodes("a") %>% html_attr("href") %>% head(20) # return first 20 results
node("section") %>% html_nodes("a") %>% html_text() %>% head(20) # return first 20 results
node("a") %>% head(20) # return first 20 results
node("") %>% html_nodes("a") %>% html_attr("href") %>% head(20) # return first 20 results
node(" ") %>% html_nodes("a") %>% html_attr("href") %>% head(20) # return first 20 results
node("level2") %>% html_nodes("a") %>% html_attr("href") %>% head(20) # return first 20 results
node("main") %>% html_nodes("a") %>% html_attr("href") %>% head(20) # return first 20 results
node("main") %>% html_nodes("a") %>% html_text() %>% head(20) # return first 20 results
require(magrittr)
node("main") %>%
html_nodes("a") %>%
extract2(3) %>% # get the 3rd scraped term
html_text()
node("main") %>%
html_nodes("a") %>%
.[[3]] %>% # get the 3rd scraped term
html_text()
node("img") %>% html_attr("src") %>% image_read()
require(magick)
node("img") %>% html_attr("src") %>% image_read()
url <- "https://www.imdb.com/title/tt0094625/?ref_=fn_al_tt_1"
require(magick)
require(magrittr)
node("img") %>% html_attr("src") %>% extract2(3) %>% image_read()
rmarkdown::render_site()
options(width=100)
knitr::opts_chunk$set(
eval = T, # run all code
echo = TRUE, # show code chunks in output
tidy = TRUE, # make output as tidy
message = FALSE,  # mask all messages
warning = FALSE, # mask all warnings
comment = "",
tidy.opts=list(width.cutoff=100), # set width of code chunks in output
size="small" # set code chunk size
)
packages <- c("ggplot2","ggthemes","dplyr","tidyverse","zoo","RColorBrewer","viridis","plyr")
if (require(packages)) {
install.packages(packages,dependencies = T)
require(packages)
# load tvthemes
devtools::install_github("Ryo-N7/tvthemes")
}
lapply(packages,library,character.only=T)
require(rvest)
require(xml2)
url <- "https://r4ds.had.co.nz/data-visualisation.html" # url to scrape
url %>% read_html # scrape HTML text and data
# scraping the first node only for third order headings ("h3")
url %>% read_html %>% html_node("h3") # get only first section heading
# scraping all nodes in the webpage
url %>% read_html %>% html_nodes("h3") # get section headings
url %>% read_html %>% html_nodes("img") # get just images
node <- function(n){
url %>% read_html %>% html_nodes(n)  # n = user defined node
}
node("h1") # get first order headings
# now pull just the text without the HTML elements
node("h2") %>% html_text(trim=T)
node("strong") %>% html_text(trim=T) # get just bold text
node("code") %>% html_text(trim = T) %>% unique %>% head(10)
node("blockquote") %>% html_text(trim = T)
url <- "https://www.imdb.com/title/tt0094625/?ref_=fn_al_tt_1"
url %>% read_html %>% html_table(trim = T)  # get just tables
url <- "https://r4ds.had.co.nz/index.html"
node("a") %>% head(20) # return first 20 results
node("main") %>% html_nodes("a") %>% html_attr("href") %>% head(20) # return first 20 results
node("main") %>% html_nodes("a") %>% html_text() %>% head(20) # return first 20 results
require(magrittr)
node("main") %>%
html_nodes("a") %>%
extract2(3) %>% # get the 3rd scraped term
html_text()
node("main") %>%
html_nodes("a") %>%
.[[3]] %>% # get the 3rd scraped term
html_text()
# to return a range/sequence
node("main") %>%
html_nodes("a") %>%
.[1:3] %>% # get the 3rd scraped term
html_text()
# to return a range/sequence
node("main") %>%
html_nodes("a") %>%
.[1:10] %>% # get the 3rd scraped term
html_text()
# to return a range/sequence
node("main") %>%
html_nodes("a") %>%
.[3:6] %>% # get the 3rd scraped term
html_text()
rmarkdown::render_site()
url %>% read_html() %>%
xml_find_first(".//li")
url %>% read_html() %>%
xml_find_first(".//ul")
url %>% read_html() %>%
xml_find_first(".//li")
url %>% read_html() %>%
xml_find_all(".//li")
url %>% read_html() %>%
xml_find_all(".//table")
url %>% read_html() %>%
xml_find_all(".//table")
url
options(width=100)
knitr::opts_chunk$set(
eval = T, # run all code
echo = TRUE, # show code chunks in output
tidy = TRUE, # make output as tidy
message = FALSE,  # mask all messages
warning = FALSE, # mask all warnings
comment = "",
tidy.opts=list(width.cutoff=100), # set width of code chunks in output
size="small" # set code chunk size
)
packages <- c("ggplot2","ggthemes","dplyr","tidyverse","zoo","RColorBrewer","viridis","plyr")
if (require(packages)) {
install.packages(packages,dependencies = T)
require(packages)
# load tvthemes
devtools::install_github("Ryo-N7/tvthemes")
}
lapply(packages,library,character.only=T)
require(rvest)
require(xml2)
url <- "https://r4ds.had.co.nz/data-visualisation.html" # url to scrape
url %>% read_html # scrape HTML text and data
# scraping the first node only for third order headings ("h3")
url %>% read_html %>% html_node("h3") # get only first section heading
# scraping all nodes in the webpage
url %>% read_html %>% html_nodes("h3") # get section headings
url %>% read_html %>% html_nodes("img") # get just images
node <- function(n){
url %>% read_html %>% html_nodes(n)  # n = user defined node
}
node("h1") # get first order headings
# now pull just the text without the HTML elements
node("h2") %>% html_text(trim=T)
node("strong") %>% html_text(trim=T) # get just bold text
node("code") %>% html_text(trim = T) %>% unique %>% head(10)
node("blockquote") %>% html_text(trim = T)
url <- "https://www.imdb.com/title/tt0094625/?ref_=fn_al_tt_1"
url %>% read_html %>% html_table(trim = T)  # get just tables
url <- "https://r4ds.had.co.nz/index.html"
node("a") %>% head(20) # return first 20 results
node("main") %>% html_nodes("a") %>% html_attr("href") %>% head(20) # return first 20 results
node("main") %>% html_nodes("a") %>% html_text() %>% head(20) # return first 20 results
require(magrittr)
node("main") %>%
html_nodes("a") %>%
extract2(3) %>% # get the 3rd scraped term
html_text()
node("main") %>%
html_nodes("a") %>%
.[[3]] %>% # get the 3rd scraped term
html_text()
# to return a range/sequence
node("main") %>%
html_nodes("a") %>%
.[3:6] %>% # get range
html_text()
require(magick)
node("img") %>% html_attr("src") %>% image_read()
# find all XML returning 'table'
url %>% read_html() %>%
xml_find_all(".//table")
# find the first XML returning 'list'
url %>% read_html() %>%
xml_find_first(".//li")
url %>% read_html() %>%
xml_find_all(".//tables")
url %>% read_html() %>%
xml_find_all(".//img")
url %>% read_html() %>%
xml_find_all(".//a")
url %>% read_html() %>%
xml_find_all(".//href")
url %>% read_html() %>%
xml_find_first(".//li")
rmarkdown::render_site()
url %>% read_html() %>%
xml_find_all(".//li")
# find all XML returning 'a' tags
url %>% read_html() %>%
xml_find_all(".//a")
url %>% read_html() %>%
xml_find_all(".//a") %>% html_text
url %>% read_html() %>%
xml_find_all(".//a") %>%
html_text %>% head()
url %>% read_html() %>%
xml_find_all(".//a") %>%
html_text %>% head()
url %>% read_html() %>%
xml_find_all(".//a")
rmarkdown::render_site()
txt_s <- txt_split[5]
txt_s %>% str_dup(3) # duplicate string n number of times (3)
rmarkdown::render_site()
ls()
list=ls()
list
rm(list=ls())
ls()
options(width=100)
knitr::opts_chunk$set(
eval = T, # run all code
echo = TRUE, # show code chunks in output
tidy = TRUE, # make output as tidy
message = FALSE,  # mask all messages
warning = FALSE, # mask all warnings
comment = "",
tidy.opts=list(width.cutoff=100), # set width of code chunks in output
size="small" # set code chunk size
)
packages <- c("ggplot2","ggthemes","dplyr","tidyverse","zoo","RColorBrewer","viridis","plyr")
if (require(packages)) {
install.packages(packages,dependencies = T)
require(packages)
# load tvthemes
devtools::install_github("Ryo-N7/tvthemes")
}
lapply(packages,library,character.only=T)
browser()
q
pacman::p_load(dplyr)
pacman::p_load(dplyr,tibble)
options(error = recover)
pacman::p_load(rlang)
last_trace()
last_error()
xx <- 1:10
xx <- LETTERS[1:10]
xx %>% mean
xx %>% rownames()
xx %>% rowMeans()
browser() # move incrementally through code lines
error
?debug
?testthat
library(testthat)
a <- 9
expect_that(a, is_less_than(10))
expect_lt(a, 10)
expect_lt
expect_lt(a, 10)
expect_that(a, is_less_than(10))
capture_error(xx %>% rowMeans())
capture_warning(xx %>% rowMeans())
capture_warning()
?test_that()
?expect_equal
expect_equal(sin(pi / 4), 1 / sqrt(2)
)
mean[1]
mean[[1]]
mean$a
sum[1]
sum$a
sum$1
sum$a
sum
data
?tags()
?httr:with_verbose()
?with_verbose()
options(future.debug=T)
options(future.debug=F)
?qq_show()
require(usethis)
use_testthat()
xx <- LETTERS[1:10]
browser() # enter browsing mode before error
xx %>% rowMeans() # this is erroneous code
xx <- LETTERS[1:10]
xx %>% rowMeans() # this is erroneous code
?stderr
